name: Generate Predictions

on:
  workflow_call:
    inputs:
      numerator:
        required: true
        type: string
      denominator:
        required: true
        type: string
      model-id:
        required: true
        type: string
      sha:
        required: true
        type: string
      sample-only:
        required: false
        type: string

jobs:
  infer-and-upload:
    runs-on: ubuntu-latest
    steps:
      - name: Print system details
        run: sudo lshw -short

      - name: checkout repo content
        uses: actions/checkout@v4


      - name: Print contents of the current directory
        run: ls -la

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@master
        with:
            aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY }}
            aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
            aws-region: eu-central-1

      - name: Install make
        run: sudo apt-get update && sudo apt-get install -y make

      - name: Set up Python environment
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'
          cache: 'poetry'
      - run: make install-prod
      
      # we need this step as ersilia will use the default conda environment to run the example model during `ersilia serve`
      # could get around this eventually if we only use conda for env management, but there are complexities around referencing a dev
      # package from within our own package which poetry makes a lot easier
      - name: set up conda
        run: |
          cd ersilia
          echo $CONDA/bin >> $GITHUB_PATH
          source $CONDA/etc/profile.d/conda.sh
          conda install -y python=3.10
          conda init
          python -m pip install -e .[test]
          pip install pyairtable
          cd ..

      - name: Activate virtual environment
        run: source .venv/bin/activate
      
      - name: Restore cached Docker image
        id: cache
        uses: actions/cache@v3
        with:
          path: /tmp/${{ inputs.model-id }}.tar
          key: ${{ runner.os }}-docker-${{ inputs.model-id }}
          restore-keys: |
            ${{ runner.os }}-docker-${{ inputs.model-id }}

      - name: Load Docker image
        if: steps.cache.outputs.cache-hit != 'true'
        run: |
          docker load -i /tmp/${{ inputs.model-id }}.tar

      - name: Run Python script to generate predictions and upload to S3
        env:
          INPUT_MODEL_ID: ${{ inputs.model-id }}
          INPUT_SHA: ${{ inputs.SHA }}
          INPUT_NUMERATOR: ${{ inputs.numerator }}
          INPUT_DENOMINATOR: ${{ inputs.denominator }}
          INPUT_SAMPLE_ONLY: ${{ inputs.sample-only }}
        run: .venv/bin/python scripts/generate_predictions.py --env prod